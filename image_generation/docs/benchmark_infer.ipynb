{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "## Pytorch vs ONNX\n",
    "All experiments are conducted on a single GeForce GTX 1070 GPU with 8G GPU memory. Downloaded models are saved in `~/.cache/huggingface/hub/`.\n",
    "> Most acceleration techniques are not support on old GPUs. Thus, we only provide the results of naive inference with two inference engines. More advanced acceleration tips for Pytorch can refer to the [fast diffusion tutorial](https://huggingface.co/docs/diffusers/tutorials/fast_diffusion). More acceleration tips for ONNX can refer to the [Optimum Inference tutorial](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models).\n",
    "\n",
    "```bash\n",
    "python -W ignore ./image_generation/benchmark/inference.py pytorch # benchmark pytorch\n",
    "python -W ignore ./image_generation/benchmark/inference.py onnx # benchmark onnx\n",
    "```\n",
    "\n",
    "|Metric|Pytorch (GPU)|ONNX (CPU)|\n",
    "|---|---|---|\n",
    "|Latency (s)|33|400|\n",
    "\n",
    "## References\n",
    "1. AutoPipeline: https://huggingface.co/docs/diffusers/tutorials/autopipeline\n",
    "2. Accelerate inference of text-to-image diffusion models: https://huggingface.co/docs/diffusers/tutorials/fast_diffusion\n",
    "3. Optimum Inference with ONNX Runtime: https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
